{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9045cf71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-21 11:59:19.437448: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-21 11:59:19.476055: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Configuring GPU and precision...\n",
      "   GPU configured: ['/physical_device:GPU:0']\n",
      "   Mixed precision: float16 enabled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-21 11:59:20.486167: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from utils.pkl_preprocessor import PickleBatchLoader\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import pandas as pd\n",
    "\n",
    "# import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from utils.cnn import (\n",
    "    train_cnn_with_batch_loader,\n",
    "    clear_gpu_memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ee8d5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cnn_incrementally(batch_loader):\n",
    "    \"\"\"\n",
    "    Function to train CNN on batches from a loader.\n",
    "    This integrates CNN training into your existing comparison framework.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting CNN Training ---\")\n",
    "\n",
    "    # Train CNN with the same batch loader\n",
    "    accuracy = train_cnn_with_batch_loader(\n",
    "        batch_loader,\n",
    "        epochs=30,  # Reduced epochs for faster comparison\n",
    "        batch_size=128,  # Smaller batch size for memory efficiency\n",
    "        patience=8,  # Early stopping patience\n",
    "        learning_rate=0.001,\n",
    "    )\n",
    "\n",
    "    # Clear GPU memory after training\n",
    "    clear_gpu_memory()\n",
    "\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b4a1cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost_incrementally(batch_loader):\n",
    "    \"\"\"\n",
    "    Function to train XGBoost incrementally on batches from a loader.\n",
    "    This version corrects the incremental training logic.\n",
    "    \"\"\"\n",
    "    # 1. Instantiate the classifier ONCE before the loop.\n",
    "    model = xgb.XGBClassifier(\n",
    "        objective=\"binary:logistic\",\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        n_estimators=100,\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    # This will hold the trained Booster object from the previous iteration.\n",
    "    trained_booster = None\n",
    "\n",
    "    print(\"--- Starting Incremental XGBoost Training ---\")\n",
    "\n",
    "    # 2. Loop through batches\n",
    "    for i, (X_batch, y_batch) in enumerate(batch_loader.batch_generator()):\n",
    "        print(f\"  Training XGBoost on batch {i + 1}/{len(batch_loader)}...\")\n",
    "\n",
    "        # For the first batch, trained_booster is None.\n",
    "        # For subsequent batches, it's the model from the last step.\n",
    "        model.fit(X_batch, y_batch, xgb_model=trained_booster)\n",
    "\n",
    "        # 3. Get the underlying booster to pass to the next iteration\n",
    "        trained_booster = model.get_booster()\n",
    "\n",
    "    # 4. Evaluation on the hold-out test set\n",
    "    print(\"  Evaluating final XGBoost model...\")\n",
    "    X_test, y_test = batch_loader.get_test_set()\n",
    "    predictions = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "    return accuracy * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2de6c167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_random_forest_incrementally(batch_loader):\n",
    "    \"\"\"\n",
    "    Function to train Random Forest using warm_start on batches.\n",
    "    \"\"\"\n",
    "    # warm_start=True is key for incremental additions\n",
    "    rf_model = RandomForestClassifier(n_estimators=5, random_state=42, warm_start=True)\n",
    "    print(\"--- Starting Incremental Random Forest Training ---\")\n",
    "\n",
    "    # Training loop\n",
    "    for i, (X_batch, y_batch) in enumerate(batch_loader.batch_generator()):\n",
    "        print(f\"  Training Random Forest on batch {i + 1}/{len(batch_loader)}...\")\n",
    "        rf_model.fit(X_batch, y_batch)\n",
    "        # Increase the number of estimators for the next batch\n",
    "        rf_model.n_estimators += 5\n",
    "\n",
    "    # Evaluation on the hold-out test set\n",
    "    print(\"  Evaluating final Random Forest model...\")\n",
    "    X_test, y_test = batch_loader.get_test_set()\n",
    "    predictions = rf_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    return accuracy * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb809cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gradient_boosting_incrementally(batch_loader):\n",
    "    \"\"\"\n",
    "    Function to train Gradient Boosting using warm_start on batches.\n",
    "    \"\"\"\n",
    "    # warm_start=True is key for incremental additions\n",
    "    gb_model = GradientBoostingClassifier(\n",
    "        n_estimators=5, learning_rate=0.1, max_depth=6, random_state=42, warm_start=True\n",
    "    )\n",
    "    print(\"--- Starting Incremental Gradient Boosting Training ---\")\n",
    "\n",
    "    # Training loop\n",
    "    for i, (X_batch, y_batch) in enumerate(batch_loader.batch_generator()):\n",
    "        print(f\"  Training Gradient Boosting on batch {i + 1}/{len(batch_loader)}...\")\n",
    "        gb_model.fit(X_batch, y_batch)\n",
    "        # Increase the number of estimators for the next batch\n",
    "        gb_model.n_estimators += 5\n",
    "\n",
    "    # Evaluation on the hold-out test set\n",
    "    print(\"  Evaluating final Gradient Boosting model...\")\n",
    "    X_test, y_test = batch_loader.get_test_set()\n",
    "    predictions = gb_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    return accuracy * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "695ab453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_algorithm(func, *args, **kwargs):\n",
    "    \"\"\"Utility function to time algorithm execution\"\"\"\n",
    "    start_time = time.time()\n",
    "    result = func(*args, **kwargs)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    return result, execution_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ce1700",
   "metadata": {},
   "source": [
    "# Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ee7c6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Testing Round 4 of SM4 with Deltas: [105, 106, 128]\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 100096\n",
    "TEST_ROUND = 4\n",
    "TEST_DELTAS = [105, 106, 128]\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"Testing Round {TEST_ROUND} of SM4 with Deltas: {TEST_DELTAS}\")\n",
    "print(f\"{'=' * 60}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d74b8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_data = {\n",
    "    \"delta\": [],\n",
    "    \"xgboost_accuracy\": [],\n",
    "    \"random_forest_accuracy\": [],\n",
    "    \"gradient_boosting_accuracy\": [],\n",
    "    \"cnn_accuracy\": [],  # Add this line\n",
    "    \"xgboost_time\": [],\n",
    "    \"random_forest_time\": [],\n",
    "    \"gradient_boosting_time\": [],\n",
    "    \"cnn_time\": [],  # Add this line\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95c00564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Processing Delta 105 from file: dataset_pkl_round_4/SM4_4_round_delta-105_combined.pkl\n",
      "==================================================\n",
      "\n",
      "Loading data from dataset_pkl_round_4/SM4_4_round_delta-105_combined.pkl...\n",
      "Preprocessing hold-out test set...\n",
      "Loader initialized. Ready to generate batches.\n",
      "Training XGBoost...\n",
      "--- Starting Incremental XGBoost Training ---\n",
      "  Training XGBoost on batch 1/2...\n",
      "  Training XGBoost on batch 2/2...\n",
      "  Evaluating final XGBoost model...\n",
      "Training Random Forest...\n",
      "--- Starting Incremental Random Forest Training ---\n",
      "  Training Random Forest on batch 1/2...\n",
      "  Training Random Forest on batch 2/2...\n",
      "  Evaluating final Random Forest model...\n",
      "Training Gradient Boosting...\n",
      "--- Starting Incremental Gradient Boosting Training ---\n",
      "  Training Gradient Boosting on batch 1/2...\n",
      "  Training Gradient Boosting on batch 2/2...\n",
      "  Evaluating final Gradient Boosting model...\n",
      "Training CNN...\n",
      "--- Starting CNN Training ---\n",
      "üöÄ Training CNN with batch loader...\n",
      "üìÇ Loading data from batch loader for CNN...\n",
      "   Collecting batches...\n",
      "   Loaded batch 1/2\n",
      "   Loaded batch 2/2\n",
      "   Combined data shape: X=(160153, 256), y=(160153,)\n",
      "   CNN input shapes - Train: (128122, 256, 1), Test: (32031, 256, 1)\n",
      "   Class distribution: [80076 80077]\n",
      "   Input shape for CNN: (256, 1)\n",
      "üß† Creating optimized CNN model for batch training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1755757787.941211  112205 gpu_process_state.cc:208] Using CUDA malloc Async allocator for GPU: 0\n",
      "I0000 00:00:1755757787.944166  112205 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4140 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Optimized CNN Model created!\n",
      "   Total parameters: 1,651,233\n",
      "   Estimated memory usage: ~3.1 MB\n",
      "   Starting training with batch_size=128, epochs=30\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-21 11:59:52.623697: I external/local_xla/xla/service/service.cc:163] XLA service 0x7f2e800165c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-08-21 11:59:52.623717: I external/local_xla/xla/service/service.cc:171]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Laptop GPU, Compute Capability 8.6\n",
      "2025-08-21 11:59:52.730314: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-08-21 11:59:54.042685: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91200\n",
      "2025-08-21 11:59:54.617953: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-08-21 11:59:54.618140: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-08-21 11:59:54.749244: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4652', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-08-21 11:59:56.873378: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:361] gpu_async_0 cuMemAllocAsync failed to allocate 10847604752 bytes: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      " Reported by CUDA: Free memory/Total memory: 1594163200/6086262784\n",
      "2025-08-21 11:59:56.873418: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:366] Stats: Limit:                      4341432320\n",
      "InUse:                       251112273\n",
      "MaxInUse:                    323408369\n",
      "NumAllocs:                        1201\n",
      "MaxAllocSize:                131196928\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2025-08-21 11:59:56.873437: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:70] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;\n",
      "2025-08-21 11:59:56.873439: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 1, 1\n",
      "2025-08-21 11:59:56.873440: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 4, 33\n",
      "2025-08-21 11:59:56.873441: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 8, 8\n",
      "2025-08-21 11:59:56.873442: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 16, 8\n",
      "2025-08-21 11:59:56.873442: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 128, 11\n",
      "2025-08-21 11:59:56.873443: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 192, 11\n",
      "2025-08-21 11:59:56.873444: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 256, 6\n",
      "2025-08-21 11:59:56.873445: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 384, 7\n",
      "2025-08-21 11:59:56.873446: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 512, 14\n",
      "2025-08-21 11:59:56.873447: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 896, 2\n",
      "2025-08-21 11:59:56.873448: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 1024, 17\n",
      "2025-08-21 11:59:56.873449: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 1028, 1\n",
      "2025-08-21 11:59:56.873449: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 2048, 6\n",
      "2025-08-21 11:59:56.873450: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 30720, 2\n",
      "2025-08-21 11:59:56.873451: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 49152, 2\n",
      "2025-08-21 11:59:56.873452: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 55296, 2\n",
      "2025-08-21 11:59:56.873453: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 131072, 3\n",
      "2025-08-21 11:59:56.873454: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 172032, 2\n",
      "2025-08-21 11:59:56.873455: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 229376, 2\n",
      "2025-08-21 11:59:56.873456: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 256248, 1\n",
      "2025-08-21 11:59:56.873457: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 524288, 4\n",
      "2025-08-21 11:59:56.873458: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 1024976, 1\n",
      "2025-08-21 11:59:56.873459: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 1441792, 2\n",
      "2025-08-21 11:59:56.873460: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 3407872, 2\n",
      "2025-08-21 11:59:56.873461: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 16863232, 2\n",
      "2025-08-21 11:59:56.873461: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 18874368, 1\n",
      "2025-08-21 11:59:56.873462: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 19922944, 1\n",
      "2025-08-21 11:59:56.873466: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 32799744, 1\n",
      "2025-08-21 11:59:56.873467: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 131196928, 1\n",
      "2025-08-21 11:59:56.873470: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:106] CU_MEMPOOL_ATTR_RESERVED_MEM_CURRENT: 4328521728\n",
      "2025-08-21 11:59:56.873471: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:108] CU_MEMPOOL_ATTR_USED_MEM_CURRENT: 251112273\n",
      "2025-08-21 11:59:56.873474: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:109] CU_MEMPOOL_ATTR_RESERVED_MEM_HIGH: 5905580032\n",
      "2025-08-21 11:59:56.873476: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:110] CU_MEMPOOL_ATTR_USED_MEM_HIGH: 323408369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  17/1001\u001b[0m \u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.4922 - loss: 1.2273"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1755757804.484066  112598 device_compiler.h:196] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 998/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4997 - loss: 0.9363"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-21 12:00:15.827018: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-08-21 12:00:15.827187: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-08-21 12:00:15.958576: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4652', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.4997 - loss: 0.9362"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-21 12:00:25.311841: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'input_convert_reduce_select_fusion', 20 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2025-08-21 12:00:27.885966: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 25ms/step - accuracy: 0.5033 - loss: 0.8973 - val_accuracy: 0.5050 - val_loss: 0.8662 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.5199 - loss: 0.8352 - val_accuracy: 0.5163 - val_loss: 0.8047 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.5482 - loss: 0.7822 - val_accuracy: 0.5017 - val_loss: 0.7881 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - accuracy: 0.5856 - loss: 0.7533 - val_accuracy: 0.5150 - val_loss: 0.7792 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 12ms/step - accuracy: 0.6091 - loss: 0.7381 - val_accuracy: 0.5075 - val_loss: 0.7836 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 12ms/step - accuracy: 0.6317 - loss: 0.7243 - val_accuracy: 0.5284 - val_loss: 0.7838 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 12ms/step - accuracy: 0.7297 - loss: 0.6313 - val_accuracy: 0.5918 - val_loss: 0.7579 - learning_rate: 0.0010\n",
      "Epoch 8/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - accuracy: 0.8537 - loss: 0.4844 - val_accuracy: 0.8150 - val_loss: 0.5661 - learning_rate: 0.0010\n",
      "Epoch 9/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - accuracy: 0.8723 - loss: 0.4516 - val_accuracy: 0.8774 - val_loss: 0.4318 - learning_rate: 0.0010\n",
      "Epoch 10/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - accuracy: 0.8768 - loss: 0.4392 - val_accuracy: 0.8671 - val_loss: 0.4525 - learning_rate: 0.0010\n",
      "Epoch 11/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - accuracy: 0.8826 - loss: 0.4251 - val_accuracy: 0.8412 - val_loss: 0.4821 - learning_rate: 0.0010\n",
      "Epoch 12/30\n",
      "\u001b[1m1000/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8842 - loss: 0.4194\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - accuracy: 0.8844 - loss: 0.4184 - val_accuracy: 0.8673 - val_loss: 0.4398 - learning_rate: 0.0010\n",
      "Epoch 13/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - accuracy: 0.8989 - loss: 0.3791 - val_accuracy: 0.8711 - val_loss: 0.4265 - learning_rate: 5.0000e-04\n",
      "Epoch 14/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - accuracy: 0.9013 - loss: 0.3594 - val_accuracy: 0.8597 - val_loss: 0.4465 - learning_rate: 5.0000e-04\n",
      "Epoch 15/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - accuracy: 0.9029 - loss: 0.3495 - val_accuracy: 0.8556 - val_loss: 0.4257 - learning_rate: 5.0000e-04\n",
      "Epoch 16/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - accuracy: 0.9046 - loss: 0.3401 - val_accuracy: 0.8823 - val_loss: 0.3816 - learning_rate: 5.0000e-04\n",
      "Epoch 17/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - accuracy: 0.9048 - loss: 0.3371 - val_accuracy: 0.8674 - val_loss: 0.4114 - learning_rate: 5.0000e-04\n",
      "Epoch 18/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - accuracy: 0.9066 - loss: 0.3308 - val_accuracy: 0.8384 - val_loss: 0.4731 - learning_rate: 5.0000e-04\n",
      "Epoch 19/30\n",
      "\u001b[1m 998/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9073 - loss: 0.3308\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - accuracy: 0.9070 - loss: 0.3297 - val_accuracy: 0.8549 - val_loss: 0.4189 - learning_rate: 5.0000e-04\n",
      "Epoch 20/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - accuracy: 0.9149 - loss: 0.3083 - val_accuracy: 0.8715 - val_loss: 0.3883 - learning_rate: 2.5000e-04\n",
      "Epoch 21/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - accuracy: 0.9180 - loss: 0.2990 - val_accuracy: 0.8776 - val_loss: 0.3686 - learning_rate: 2.5000e-04\n",
      "Epoch 22/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - accuracy: 0.9187 - loss: 0.2918 - val_accuracy: 0.8778 - val_loss: 0.3676 - learning_rate: 2.5000e-04\n",
      "Epoch 23/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - accuracy: 0.9186 - loss: 0.2885 - val_accuracy: 0.8749 - val_loss: 0.3877 - learning_rate: 2.5000e-04\n",
      "Epoch 24/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - accuracy: 0.9190 - loss: 0.2855 - val_accuracy: 0.8764 - val_loss: 0.3790 - learning_rate: 2.5000e-04\n",
      "Epoch 24: early stopping\n",
      "Restoring model weights from the end of the best epoch: 16.\n",
      "üìä CNN Training completed!\n",
      "   Final Test Accuracy: 0.8823\n",
      "‚úÖ GPU memory cleared\n",
      "\n",
      "Delta 105 Results:\n",
      "XGBoost - Accuracy: 99.9476%, Time: 2.45s\n",
      "Random Forest - Accuracy: 81.3457%, Time: 2.48s\n",
      "Gradient Boosting - Accuracy: 95.1347%, Time: 20.82s\n",
      "CNN - Accuracy: 88.2333%, Time: 294.02s\n",
      "\n",
      "==================================================\n",
      "Processing Delta 106 from file: dataset_pkl_round_4/SM4_4_round_delta-106_combined.pkl\n",
      "==================================================\n",
      "\n",
      "Loading data from dataset_pkl_round_4/SM4_4_round_delta-106_combined.pkl...\n",
      "Preprocessing hold-out test set...\n",
      "Loader initialized. Ready to generate batches.\n",
      "Training XGBoost...\n",
      "--- Starting Incremental XGBoost Training ---\n",
      "  Training XGBoost on batch 1/2...\n",
      "  Training XGBoost on batch 2/2...\n",
      "  Evaluating final XGBoost model...\n",
      "Training Random Forest...\n",
      "--- Starting Incremental Random Forest Training ---\n",
      "  Training Random Forest on batch 1/2...\n",
      "  Training Random Forest on batch 2/2...\n",
      "  Evaluating final Random Forest model...\n",
      "Training Gradient Boosting...\n",
      "--- Starting Incremental Gradient Boosting Training ---\n",
      "  Training Gradient Boosting on batch 1/2...\n",
      "  Training Gradient Boosting on batch 2/2...\n",
      "  Evaluating final Gradient Boosting model...\n",
      "Training CNN...\n",
      "--- Starting CNN Training ---\n",
      "üöÄ Training CNN with batch loader...\n",
      "üìÇ Loading data from batch loader for CNN...\n",
      "   Collecting batches...\n",
      "   Loaded batch 1/2\n",
      "   Loaded batch 2/2\n",
      "   Combined data shape: X=(160153, 256), y=(160153,)\n",
      "   CNN input shapes - Train: (128122, 256, 1), Test: (32031, 256, 1)\n",
      "   Class distribution: [80076 80077]\n",
      "   Input shape for CNN: (256, 1)\n",
      "üß† Creating optimized CNN model for batch training...\n",
      "‚úÖ Optimized CNN Model created!\n",
      "   Total parameters: 1,651,233\n",
      "   Estimated memory usage: ~3.1 MB\n",
      "   Starting training with batch_size=128, epochs=30\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-21 12:05:12.605935: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-08-21 12:05:12.606128: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-08-21 12:05:13.391825: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_9195', 76 bytes spill stores, 76 bytes spill loads\n",
      "\n",
      "2025-08-21 12:05:15.003339: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:361] gpu_async_0 cuMemAllocAsync failed to allocate 10842275840 bytes: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      " Reported by CUDA: Free memory/Total memory: 1575288832/6086262784\n",
      "2025-08-21 12:05:15.003357: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:366] Stats: Limit:                      4341432320\n",
      "InUse:                       276348425\n",
      "MaxInUse:                    357891993\n",
      "NumAllocs:                      225336\n",
      "MaxAllocSize:                131196928\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2025-08-21 12:05:15.003465: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:70] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;\n",
      "2025-08-21 12:05:15.003469: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 1, 1\n",
      "2025-08-21 12:05:15.003471: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 4, 9039\n",
      "2025-08-21 12:05:15.003472: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 8, 8\n",
      "2025-08-21 12:05:15.003473: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 16, 16\n",
      "2025-08-21 12:05:15.003474: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 128, 33\n",
      "2025-08-21 12:05:15.003475: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 192, 33\n",
      "2025-08-21 12:05:15.003476: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 256, 17\n",
      "2025-08-21 12:05:15.003477: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 384, 18\n",
      "2025-08-21 12:05:15.003478: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 512, 42\n",
      "2025-08-21 12:05:15.003479: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 896, 5\n",
      "2025-08-21 12:05:15.003480: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 1024, 50\n",
      "2025-08-21 12:05:15.003481: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 1028, 1\n",
      "2025-08-21 12:05:15.003482: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 2048, 17\n",
      "2025-08-21 12:05:15.003483: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 30720, 5\n",
      "2025-08-21 12:05:15.003484: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 49152, 5\n",
      "2025-08-21 12:05:15.003485: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 55296, 5\n",
      "2025-08-21 12:05:15.003486: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 131072, 6\n",
      "2025-08-21 12:05:15.003487: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 172032, 5\n",
      "2025-08-21 12:05:15.003488: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 229376, 5\n",
      "2025-08-21 12:05:15.003489: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 256248, 1\n",
      "2025-08-21 12:05:15.003490: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 524288, 10\n",
      "2025-08-21 12:05:15.003490: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 1024976, 1\n",
      "2025-08-21 12:05:15.003491: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 1441792, 5\n",
      "2025-08-21 12:05:15.003492: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 3407872, 5\n",
      "2025-08-21 12:05:15.003493: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 16949248, 2\n",
      "2025-08-21 12:05:15.003494: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 20971520, 1\n",
      "2025-08-21 12:05:15.003495: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 23068672, 1\n",
      "2025-08-21 12:05:15.003496: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 32799744, 1\n",
      "2025-08-21 12:05:15.003497: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 131196928, 1\n",
      "2025-08-21 12:05:15.003500: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:106] CU_MEMPOOL_ATTR_RESERVED_MEM_CURRENT: 4328521728\n",
      "2025-08-21 12:05:15.003501: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:108] CU_MEMPOOL_ATTR_USED_MEM_CURRENT: 276348425\n",
      "2025-08-21 12:05:15.003503: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:109] CU_MEMPOOL_ATTR_RESERVED_MEM_HIGH: 5905580032\n",
      "2025-08-21 12:05:15.003504: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:110] CU_MEMPOOL_ATTR_USED_MEM_HIGH: 357891993\n",
      "2025-08-21 12:05:15.111578: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:361] gpu_async_0 cuMemAllocAsync failed to allocate 13010731008 bytes: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      " Reported by CUDA: Free memory/Total memory: 1575288832/6086262784\n",
      "2025-08-21 12:05:15.111596: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:366] Stats: Limit:                      4341432320\n",
      "InUse:                       278560265\n",
      "MaxInUse:                    357891993\n",
      "NumAllocs:                      225369\n",
      "MaxAllocSize:                131196928\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2025-08-21 12:05:15.111675: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:70] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;\n",
      "2025-08-21 12:05:15.111702: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 1, 1\n",
      "2025-08-21 12:05:15.111704: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 4, 9039\n",
      "2025-08-21 12:05:15.111705: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 8, 8\n",
      "2025-08-21 12:05:15.111706: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 16, 16\n",
      "2025-08-21 12:05:15.111707: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 128, 33\n",
      "2025-08-21 12:05:15.111708: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 192, 33\n",
      "2025-08-21 12:05:15.111709: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 256, 17\n",
      "2025-08-21 12:05:15.111709: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 384, 18\n",
      "2025-08-21 12:05:15.111710: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 512, 42\n",
      "2025-08-21 12:05:15.111711: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 896, 5\n",
      "2025-08-21 12:05:15.111712: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 1024, 50\n",
      "2025-08-21 12:05:15.111713: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 1028, 1\n",
      "2025-08-21 12:05:15.111714: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 2048, 17\n",
      "2025-08-21 12:05:15.111715: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 30720, 5\n",
      "2025-08-21 12:05:15.111716: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 49152, 5\n",
      "2025-08-21 12:05:15.111717: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 55296, 5\n",
      "2025-08-21 12:05:15.111719: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 131072, 6\n",
      "2025-08-21 12:05:15.111720: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 172032, 5\n",
      "2025-08-21 12:05:15.111721: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 229376, 5\n",
      "2025-08-21 12:05:15.111722: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 256248, 1\n",
      "2025-08-21 12:05:15.111723: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 524288, 10\n",
      "2025-08-21 12:05:15.111724: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 1024976, 1\n",
      "2025-08-21 12:05:15.111725: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 1441792, 5\n",
      "2025-08-21 12:05:15.111725: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 3407872, 5\n",
      "2025-08-21 12:05:15.111726: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 17006592, 2\n",
      "2025-08-21 12:05:15.111727: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 20971520, 1\n",
      "2025-08-21 12:05:15.111728: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 25165824, 1\n",
      "2025-08-21 12:05:15.111729: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 32799744, 1\n",
      "2025-08-21 12:05:15.111730: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 131196928, 1\n",
      "2025-08-21 12:05:15.111734: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:106] CU_MEMPOOL_ATTR_RESERVED_MEM_CURRENT: 4328521728\n",
      "2025-08-21 12:05:15.111735: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:108] CU_MEMPOOL_ATTR_USED_MEM_CURRENT: 278560265\n",
      "2025-08-21 12:05:15.111736: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:109] CU_MEMPOOL_ATTR_RESERVED_MEM_HIGH: 5905580032\n",
      "2025-08-21 12:05:15.111737: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:110] CU_MEMPOOL_ATTR_USED_MEM_HIGH: 357891993\n",
      "2025-08-21 12:05:15.702368: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:361] gpu_async_0 cuMemAllocAsync failed to allocate 8749318144 bytes: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      " Reported by CUDA: Free memory/Total memory: 1575288832/6086262784\n",
      "2025-08-21 12:05:15.702384: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:366] Stats: Limit:                      4341432320\n",
      "InUse:                       289111561\n",
      "MaxInUse:                   4122279433\n",
      "NumAllocs:                      225445\n",
      "MaxAllocSize:               3841982464\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2025-08-21 12:05:15.702465: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:70] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;\n",
      "2025-08-21 12:05:15.702481: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 1, 1\n",
      "2025-08-21 12:05:15.702482: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 4, 9039\n",
      "2025-08-21 12:05:15.702484: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 8, 8\n",
      "2025-08-21 12:05:15.702485: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 16, 16\n",
      "2025-08-21 12:05:15.702486: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 128, 33\n",
      "2025-08-21 12:05:15.702487: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 192, 33\n",
      "2025-08-21 12:05:15.702488: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 256, 17\n",
      "2025-08-21 12:05:15.702489: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 384, 18\n",
      "2025-08-21 12:05:15.702490: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 512, 42\n",
      "2025-08-21 12:05:15.702491: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 896, 5\n",
      "2025-08-21 12:05:15.702492: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 1024, 50\n",
      "2025-08-21 12:05:15.702493: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 1028, 1\n",
      "2025-08-21 12:05:15.702494: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 2048, 17\n",
      "2025-08-21 12:05:15.702495: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 30720, 5\n",
      "2025-08-21 12:05:15.702496: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 49152, 5\n",
      "2025-08-21 12:05:15.702497: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 55296, 5\n",
      "2025-08-21 12:05:15.702498: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 131072, 6\n",
      "2025-08-21 12:05:15.702499: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 172032, 5\n",
      "2025-08-21 12:05:15.702500: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 229376, 5\n",
      "2025-08-21 12:05:15.702501: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 256248, 1\n",
      "2025-08-21 12:05:15.702502: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 524288, 10\n",
      "2025-08-21 12:05:15.702503: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 1024976, 1\n",
      "2025-08-21 12:05:15.702504: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 1441792, 5\n",
      "2025-08-21 12:05:15.702505: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 3407872, 5\n",
      "2025-08-21 12:05:15.702506: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 20185088, 2\n",
      "2025-08-21 12:05:15.702507: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 25165824, 2\n",
      "2025-08-21 12:05:15.702508: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 32799744, 1\n",
      "2025-08-21 12:05:15.702509: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 131196928, 1\n",
      "2025-08-21 12:05:15.702512: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:106] CU_MEMPOOL_ATTR_RESERVED_MEM_CURRENT: 4328521728\n",
      "2025-08-21 12:05:15.702513: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:108] CU_MEMPOOL_ATTR_USED_MEM_CURRENT: 289111561\n",
      "2025-08-21 12:05:15.702515: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:109] CU_MEMPOOL_ATTR_RESERVED_MEM_HIGH: 5905580032\n",
      "2025-08-21 12:05:15.702516: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:110] CU_MEMPOOL_ATTR_USED_MEM_HIGH: 4122279433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 999/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.5063 - loss: 0.9319"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-21 12:05:43.490946: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-08-21 12:05:43.491103: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-08-21 12:05:43.916757: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6836', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-08-21 12:05:44.784611: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_9195', 96 bytes spill stores, 96 bytes spill loads\n",
      "\n",
      "2025-08-21 12:05:46.243473: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:361] gpu_async_0 cuMemAllocAsync failed to allocate 10334830592 bytes: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      " Reported by CUDA: Free memory/Total memory: 1573191680/6086262784\n",
      "2025-08-21 12:05:46.243502: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:366] Stats: Limit:                      4341432320\n",
      "InUse:                       282461033\n",
      "MaxInUse:                   4122279433\n",
      "NumAllocs:                      233732\n",
      "MaxAllocSize:               3841982464\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2025-08-21 12:05:46.243600: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:70] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;\n",
      "2025-08-21 12:05:46.243603: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 1, 1\n",
      "2025-08-21 12:05:46.243605: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 4, 9635\n",
      "2025-08-21 12:05:46.243606: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 8, 8\n",
      "2025-08-21 12:05:46.243607: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 16, 16\n",
      "2025-08-21 12:05:46.243609: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 128, 44\n",
      "2025-08-21 12:05:46.243610: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 192, 44\n",
      "2025-08-21 12:05:46.243611: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 256, 22\n",
      "2025-08-21 12:05:46.243612: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 384, 22\n",
      "2025-08-21 12:05:46.243614: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 512, 56\n",
      "2025-08-21 12:05:46.243615: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 896, 6\n",
      "2025-08-21 12:05:46.243616: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 976, 1\n",
      "2025-08-21 12:05:46.243617: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 1024, 66\n",
      "2025-08-21 12:05:46.243619: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 1028, 1\n",
      "2025-08-21 12:05:46.243620: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 2048, 22\n",
      "2025-08-21 12:05:46.243621: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 30720, 6\n",
      "2025-08-21 12:05:46.243622: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 49152, 6\n",
      "2025-08-21 12:05:46.243624: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 55296, 6\n",
      "2025-08-21 12:05:46.243625: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 124928, 1\n",
      "2025-08-21 12:05:46.243626: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 131072, 6\n",
      "2025-08-21 12:05:46.243628: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 172032, 6\n",
      "2025-08-21 12:05:46.243629: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 229376, 6\n",
      "2025-08-21 12:05:46.243630: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 256248, 1\n",
      "2025-08-21 12:05:46.243632: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 524288, 12\n",
      "2025-08-21 12:05:46.243633: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 1024976, 1\n",
      "2025-08-21 12:05:46.243634: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 1441792, 6\n",
      "2025-08-21 12:05:46.243636: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 3407872, 6\n",
      "2025-08-21 12:05:46.243637: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 16949248, 2\n",
      "2025-08-21 12:05:46.243638: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 20774912, 1\n",
      "2025-08-21 12:05:46.243640: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 22773760, 1\n",
      "2025-08-21 12:05:46.243641: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 32799744, 1\n",
      "2025-08-21 12:05:46.243643: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 131196928, 1\n",
      "2025-08-21 12:05:46.243647: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:106] CU_MEMPOOL_ATTR_RESERVED_MEM_CURRENT: 4328521728\n",
      "2025-08-21 12:05:46.243649: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:108] CU_MEMPOOL_ATTR_USED_MEM_CURRENT: 282461033\n",
      "2025-08-21 12:05:46.243651: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:109] CU_MEMPOOL_ATTR_RESERVED_MEM_HIGH: 5905580032\n",
      "2025-08-21 12:05:46.243652: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:110] CU_MEMPOOL_ATTR_USED_MEM_HIGH: 4122279433\n",
      "2025-08-21 12:05:46.354818: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:361] gpu_async_0 cuMemAllocAsync failed to allocate 12401639424 bytes: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      " Reported by CUDA: Free memory/Total memory: 1573191680/6086262784\n",
      "2025-08-21 12:05:46.354836: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:366] Stats: Limit:                      4341432320\n",
      "InUse:                       284574569\n",
      "MaxInUse:                   4122279433\n",
      "NumAllocs:                      233764\n",
      "MaxAllocSize:               3841982464\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2025-08-21 12:05:46.354913: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:70] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;\n",
      "2025-08-21 12:05:46.354916: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 1, 1\n",
      "2025-08-21 12:05:46.354918: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 4, 9635\n",
      "2025-08-21 12:05:46.354919: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 8, 8\n",
      "2025-08-21 12:05:46.354920: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 16, 16\n",
      "2025-08-21 12:05:46.354922: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 128, 44\n",
      "2025-08-21 12:05:46.354924: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 192, 44\n",
      "2025-08-21 12:05:46.354925: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 256, 22\n",
      "2025-08-21 12:05:46.354927: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 384, 22\n",
      "2025-08-21 12:05:46.354929: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 512, 56\n",
      "2025-08-21 12:05:46.354930: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 896, 6\n",
      "2025-08-21 12:05:46.354932: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 976, 1\n",
      "2025-08-21 12:05:46.354934: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 1024, 66\n",
      "2025-08-21 12:05:46.354936: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 1028, 1\n",
      "2025-08-21 12:05:46.354938: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 2048, 22\n",
      "2025-08-21 12:05:46.354940: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 30720, 6\n",
      "2025-08-21 12:05:46.354942: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 49152, 6\n",
      "2025-08-21 12:05:46.354943: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 55296, 6\n",
      "2025-08-21 12:05:46.354945: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 124928, 1\n",
      "2025-08-21 12:05:46.354947: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 131072, 6\n",
      "2025-08-21 12:05:46.354949: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 172032, 6\n",
      "2025-08-21 12:05:46.354950: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 229376, 6\n",
      "2025-08-21 12:05:46.354952: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 256248, 1\n",
      "2025-08-21 12:05:46.354954: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 524288, 12\n",
      "2025-08-21 12:05:46.354956: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 1024976, 1\n",
      "2025-08-21 12:05:46.354958: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 1441792, 6\n",
      "2025-08-21 12:05:46.354959: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 3407872, 6\n",
      "2025-08-21 12:05:46.354961: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 17006592, 2\n",
      "2025-08-21 12:05:46.354963: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 20774912, 1\n",
      "2025-08-21 12:05:46.354965: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 24772608, 1\n",
      "2025-08-21 12:05:46.354966: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 32799744, 1\n",
      "2025-08-21 12:05:46.354968: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 131196928, 1\n",
      "2025-08-21 12:05:46.354973: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:106] CU_MEMPOOL_ATTR_RESERVED_MEM_CURRENT: 4328521728\n",
      "2025-08-21 12:05:46.354975: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:108] CU_MEMPOOL_ATTR_USED_MEM_CURRENT: 284574569\n",
      "2025-08-21 12:05:46.354978: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:109] CU_MEMPOOL_ATTR_RESERVED_MEM_HIGH: 5905580032\n",
      "2025-08-21 12:05:46.354980: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:110] CU_MEMPOOL_ATTR_USED_MEM_HIGH: 4122279433\n",
      "2025-08-21 12:05:46.900701: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:361] gpu_async_0 cuMemAllocAsync failed to allocate 8748924928 bytes: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      " Reported by CUDA: Free memory/Total memory: 1573191680/6086262784\n",
      "2025-08-21 12:05:46.900719: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:366] Stats: Limit:                      4341432320\n",
      "InUse:                       294929257\n",
      "MaxInUse:                   4122279433\n",
      "NumAllocs:                      233840\n",
      "MaxAllocSize:               3841982464\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2025-08-21 12:05:46.900827: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:70] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;\n",
      "2025-08-21 12:05:46.900830: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 1, 1\n",
      "2025-08-21 12:05:46.900832: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 4, 9635\n",
      "2025-08-21 12:05:46.900833: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 8, 8\n",
      "2025-08-21 12:05:46.900835: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 16, 16\n",
      "2025-08-21 12:05:46.900836: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 128, 44\n",
      "2025-08-21 12:05:46.900837: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 192, 44\n",
      "2025-08-21 12:05:46.900838: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 256, 22\n",
      "2025-08-21 12:05:46.900839: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 384, 22\n",
      "2025-08-21 12:05:46.900840: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 512, 56\n",
      "2025-08-21 12:05:46.900841: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 896, 6\n",
      "2025-08-21 12:05:46.900842: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 976, 1\n",
      "2025-08-21 12:05:46.900844: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 1024, 66\n",
      "2025-08-21 12:05:46.900845: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 1028, 1\n",
      "2025-08-21 12:05:46.900846: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 2048, 22\n",
      "2025-08-21 12:05:46.900847: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 30720, 6\n",
      "2025-08-21 12:05:46.900848: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 49152, 6\n",
      "2025-08-21 12:05:46.900850: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 55296, 6\n",
      "2025-08-21 12:05:46.900851: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 124928, 1\n",
      "2025-08-21 12:05:46.900852: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 131072, 6\n",
      "2025-08-21 12:05:46.900853: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 172032, 6\n",
      "2025-08-21 12:05:46.900854: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 229376, 6\n",
      "2025-08-21 12:05:46.900855: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 256248, 1\n",
      "2025-08-21 12:05:46.900856: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 524288, 12\n",
      "2025-08-21 12:05:46.900858: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 1024976, 1\n",
      "2025-08-21 12:05:46.900865: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 1441792, 6\n",
      "2025-08-21 12:05:46.900866: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 3407872, 6\n",
      "2025-08-21 12:05:46.900867: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 20185088, 2\n",
      "2025-08-21 12:05:46.900868: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 24772608, 2\n",
      "2025-08-21 12:05:46.900870: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 32799744, 1\n",
      "2025-08-21 12:05:46.900871: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 131196928, 1\n",
      "2025-08-21 12:05:46.900876: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:106] CU_MEMPOOL_ATTR_RESERVED_MEM_CURRENT: 4328521728\n",
      "2025-08-21 12:05:46.900878: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:108] CU_MEMPOOL_ATTR_USED_MEM_CURRENT: 294929257\n",
      "2025-08-21 12:05:46.900879: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:109] CU_MEMPOOL_ATTR_RESERVED_MEM_HIGH: 5905580032\n",
      "2025-08-21 12:05:46.900881: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:110] CU_MEMPOOL_ATTR_USED_MEM_HIGH: 4122279433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.5063 - loss: 0.9318"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-21 12:06:01.881918: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 40ms/step - accuracy: 0.5115 - loss: 0.8939 - val_accuracy: 0.5003 - val_loss: 0.8642 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.5433 - loss: 0.8309 - val_accuracy: 0.5174 - val_loss: 0.8211 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.5791 - loss: 0.7808 - val_accuracy: 0.5212 - val_loss: 0.8112 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 18ms/step - accuracy: 0.6022 - loss: 0.7487 - val_accuracy: 0.5020 - val_loss: 0.8889 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.6164 - loss: 0.7328 - val_accuracy: 0.5149 - val_loss: 0.8080 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.6478 - loss: 0.7117 - val_accuracy: 0.5591 - val_loss: 0.9088 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.8033 - loss: 0.5556 - val_accuracy: 0.8266 - val_loss: 0.5541 - learning_rate: 0.0010\n",
      "Epoch 8/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.8612 - loss: 0.4710 - val_accuracy: 0.8426 - val_loss: 0.5122 - learning_rate: 0.0010\n",
      "Epoch 9/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 18ms/step - accuracy: 0.8740 - loss: 0.4464 - val_accuracy: 0.8459 - val_loss: 0.4989 - learning_rate: 0.0010\n",
      "Epoch 10/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.8797 - loss: 0.4339 - val_accuracy: 0.8722 - val_loss: 0.4458 - learning_rate: 0.0010\n",
      "Epoch 11/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.8827 - loss: 0.4271 - val_accuracy: 0.8261 - val_loss: 0.5318 - learning_rate: 0.0010\n",
      "Epoch 12/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.8856 - loss: 0.4191 - val_accuracy: 0.8580 - val_loss: 0.4588 - learning_rate: 0.0010\n",
      "Epoch 13/30\n",
      "\u001b[1m1000/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8860 - loss: 0.4171\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 18ms/step - accuracy: 0.8866 - loss: 0.4156 - val_accuracy: 0.8731 - val_loss: 0.4557 - learning_rate: 0.0010\n",
      "Epoch 14/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 18ms/step - accuracy: 0.9005 - loss: 0.3777 - val_accuracy: 0.8920 - val_loss: 0.3720 - learning_rate: 5.0000e-04\n",
      "Epoch 15/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.9042 - loss: 0.3549 - val_accuracy: 0.8526 - val_loss: 0.4609 - learning_rate: 5.0000e-04\n",
      "Epoch 16/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.9037 - loss: 0.3482 - val_accuracy: 0.8604 - val_loss: 0.4418 - learning_rate: 5.0000e-04\n",
      "Epoch 17/30\n",
      "\u001b[1m 999/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9055 - loss: 0.3411\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.9057 - loss: 0.3388 - val_accuracy: 0.8640 - val_loss: 0.4124 - learning_rate: 5.0000e-04\n",
      "Epoch 18/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.9132 - loss: 0.3184 - val_accuracy: 0.8701 - val_loss: 0.4055 - learning_rate: 2.5000e-04\n",
      "Epoch 19/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.9155 - loss: 0.3060 - val_accuracy: 0.8616 - val_loss: 0.4119 - learning_rate: 2.5000e-04\n",
      "Epoch 20/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9175 - loss: 0.2997\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.9177 - loss: 0.2985 - val_accuracy: 0.8689 - val_loss: 0.4116 - learning_rate: 2.5000e-04\n",
      "Epoch 21/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.9206 - loss: 0.2875 - val_accuracy: 0.8688 - val_loss: 0.4022 - learning_rate: 1.2500e-04\n",
      "Epoch 22/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.9223 - loss: 0.2801 - val_accuracy: 0.8671 - val_loss: 0.3989 - learning_rate: 1.2500e-04\n",
      "Epoch 22: early stopping\n",
      "Restoring model weights from the end of the best epoch: 14.\n",
      "üìä CNN Training completed!\n",
      "   Final Test Accuracy: 0.8920\n",
      "‚úÖ GPU memory cleared\n",
      "\n",
      "Delta 106 Results:\n",
      "XGBoost - Accuracy: 99.9401%, Time: 2.68s\n",
      "Random Forest - Accuracy: 77.0823%, Time: 2.11s\n",
      "Gradient Boosting - Accuracy: 92.7995%, Time: 18.15s\n",
      "CNN - Accuracy: 89.2042%, Time: 463.25s\n",
      "\n",
      "==================================================\n",
      "Processing Delta 128 from file: dataset_pkl_round_4/SM4_4_round_delta-128_combined.pkl\n",
      "==================================================\n",
      "\n",
      "Loading data from dataset_pkl_round_4/SM4_4_round_delta-128_combined.pkl...\n",
      "Preprocessing hold-out test set...\n",
      "Loader initialized. Ready to generate batches.\n",
      "Training XGBoost...\n",
      "--- Starting Incremental XGBoost Training ---\n",
      "  Training XGBoost on batch 1/2...\n",
      "  Training XGBoost on batch 2/2...\n",
      "  Evaluating final XGBoost model...\n",
      "Training Random Forest...\n",
      "--- Starting Incremental Random Forest Training ---\n",
      "  Training Random Forest on batch 1/2...\n",
      "  Training Random Forest on batch 2/2...\n",
      "  Evaluating final Random Forest model...\n",
      "Training Gradient Boosting...\n",
      "--- Starting Incremental Gradient Boosting Training ---\n",
      "  Training Gradient Boosting on batch 1/2...\n",
      "  Training Gradient Boosting on batch 2/2...\n",
      "  Evaluating final Gradient Boosting model...\n",
      "Training CNN...\n",
      "--- Starting CNN Training ---\n",
      "üöÄ Training CNN with batch loader...\n",
      "üìÇ Loading data from batch loader for CNN...\n",
      "   Collecting batches...\n",
      "   Loaded batch 1/2\n",
      "   Loaded batch 2/2\n",
      "   Combined data shape: X=(160153, 256), y=(160153,)\n",
      "   CNN input shapes - Train: (128122, 256, 1), Test: (32031, 256, 1)\n",
      "   Class distribution: [80076 80077]\n",
      "   Input shape for CNN: (256, 1)\n",
      "üß† Creating optimized CNN model for batch training...\n",
      "‚úÖ Optimized CNN Model created!\n",
      "   Total parameters: 1,651,233\n",
      "   Estimated memory usage: ~3.1 MB\n",
      "   Starting training with batch_size=128, epochs=30\n",
      "Epoch 1/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 29ms/step - accuracy: 0.5036 - loss: 0.8960 - val_accuracy: 0.4996 - val_loss: 0.8655 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 17ms/step - accuracy: 0.5277 - loss: 0.8341 - val_accuracy: 0.5284 - val_loss: 0.8074 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.5660 - loss: 0.7773 - val_accuracy: 0.5233 - val_loss: 0.8007 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.5937 - loss: 0.7458 - val_accuracy: 0.5131 - val_loss: 0.8538 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.6137 - loss: 0.7295 - val_accuracy: 0.5245 - val_loss: 0.7834 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.6437 - loss: 0.7073 - val_accuracy: 0.7367 - val_loss: 0.6212 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.8033 - loss: 0.5556 - val_accuracy: 0.8424 - val_loss: 0.4972 - learning_rate: 0.0010\n",
      "Epoch 8/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.8621 - loss: 0.4740 - val_accuracy: 0.8572 - val_loss: 0.4641 - learning_rate: 0.0010\n",
      "Epoch 9/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.8741 - loss: 0.4462 - val_accuracy: 0.8304 - val_loss: 0.5142 - learning_rate: 0.0010\n",
      "Epoch 10/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.8796 - loss: 0.4327 - val_accuracy: 0.8547 - val_loss: 0.4585 - learning_rate: 0.0010\n",
      "Epoch 11/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.8848 - loss: 0.4230 - val_accuracy: 0.8378 - val_loss: 0.4957 - learning_rate: 0.0010\n",
      "Epoch 12/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.8879 - loss: 0.4159 - val_accuracy: 0.8421 - val_loss: 0.4916 - learning_rate: 0.0010\n",
      "Epoch 13/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.8908 - loss: 0.4075 - val_accuracy: 0.8616 - val_loss: 0.4528 - learning_rate: 0.0010\n",
      "Epoch 14/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.8910 - loss: 0.4053 - val_accuracy: 0.8617 - val_loss: 0.4554 - learning_rate: 0.0010\n",
      "Epoch 15/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.8918 - loss: 0.4019 - val_accuracy: 0.8703 - val_loss: 0.4420 - learning_rate: 0.0010\n",
      "Epoch 16/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.8942 - loss: 0.3963 - val_accuracy: 0.8715 - val_loss: 0.4309 - learning_rate: 0.0010\n",
      "Epoch 17/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.8966 - loss: 0.3922 - val_accuracy: 0.8667 - val_loss: 0.4284 - learning_rate: 0.0010\n",
      "Epoch 18/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.8976 - loss: 0.3890 - val_accuracy: 0.8601 - val_loss: 0.4595 - learning_rate: 0.0010\n",
      "Epoch 19/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.8973 - loss: 0.3891 - val_accuracy: 0.8722 - val_loss: 0.4274 - learning_rate: 0.0010\n",
      "Epoch 20/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.8994 - loss: 0.3850 - val_accuracy: 0.8249 - val_loss: 0.4983 - learning_rate: 0.0010\n",
      "Epoch 21/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.8988 - loss: 0.3848 - val_accuracy: 0.8491 - val_loss: 0.4744 - learning_rate: 0.0010\n",
      "Epoch 22/30\n",
      "\u001b[1m1000/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9012 - loss: 0.3817\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.9008 - loss: 0.3820 - val_accuracy: 0.8677 - val_loss: 0.4380 - learning_rate: 0.0010\n",
      "Epoch 23/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.9107 - loss: 0.3494 - val_accuracy: 0.8728 - val_loss: 0.4187 - learning_rate: 5.0000e-04\n",
      "Epoch 24/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.9138 - loss: 0.3303 - val_accuracy: 0.8712 - val_loss: 0.4102 - learning_rate: 5.0000e-04\n",
      "Epoch 25/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.9155 - loss: 0.3187 - val_accuracy: 0.8853 - val_loss: 0.3803 - learning_rate: 5.0000e-04\n",
      "Epoch 26/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.9160 - loss: 0.3141 - val_accuracy: 0.8518 - val_loss: 0.4475 - learning_rate: 5.0000e-04\n",
      "Epoch 27/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.9153 - loss: 0.3131 - val_accuracy: 0.8570 - val_loss: 0.4270 - learning_rate: 5.0000e-04\n",
      "Epoch 28/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.9174 - loss: 0.3074 - val_accuracy: 0.8961 - val_loss: 0.3472 - learning_rate: 5.0000e-04\n",
      "Epoch 29/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 18ms/step - accuracy: 0.9166 - loss: 0.3052 - val_accuracy: 0.8912 - val_loss: 0.3563 - learning_rate: 5.0000e-04\n",
      "Epoch 30/30\n",
      "\u001b[1m1001/1001\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.9178 - loss: 0.3042 - val_accuracy: 0.8824 - val_loss: 0.3743 - learning_rate: 5.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 28.\n",
      "üìä CNN Training completed!\n",
      "   Final Test Accuracy: 0.8961\n",
      "‚úÖ GPU memory cleared\n",
      "\n",
      "Delta 128 Results:\n",
      "XGBoost - Accuracy: 99.9550%, Time: 5.67s\n",
      "Random Forest - Accuracy: 80.3891%, Time: 2.83s\n",
      "Gradient Boosting - Accuracy: 95.9914%, Time: 23.38s\n",
      "CNN - Accuracy: 89.6132%, Time: 617.81s\n"
     ]
    }
   ],
   "source": [
    "for delta in TEST_DELTAS:\n",
    "    pickle_file = f\"dataset_pkl_round_{TEST_ROUND}/SM4_{TEST_ROUND}_round_delta-{delta}_combined.pkl\"\n",
    "\n",
    "    print(f\"\\n{'=' * 50}\")\n",
    "    print(f\"Processing Delta {delta} from file: {pickle_file}\")\n",
    "    print(f\"{'=' * 50}\\n\")\n",
    "\n",
    "    try:\n",
    "        batch_loader = PickleBatchLoader(pickle_file, batch_size=BATCH_SIZE)\n",
    "\n",
    "        # Train and time each algorithm (including CNN)\n",
    "        print(\"Training XGBoost...\")\n",
    "        xgboost_accuracy, xgboost_time = time_algorithm(\n",
    "            train_xgboost_incrementally, batch_loader\n",
    "        )\n",
    "\n",
    "        print(\"Training Random Forest...\")\n",
    "        random_forest_accuracy, random_forest_time = time_algorithm(\n",
    "            train_random_forest_incrementally, batch_loader\n",
    "        )\n",
    "\n",
    "        print(\"Training Gradient Boosting...\")\n",
    "        gradient_boosting_accuracy, gradient_boosting_time = time_algorithm(\n",
    "            train_gradient_boosting_incrementally, batch_loader\n",
    "        )\n",
    "\n",
    "        # Add CNN training\n",
    "        print(\"Training CNN...\")\n",
    "        cnn_accuracy, cnn_time = time_algorithm(train_cnn_incrementally, batch_loader)\n",
    "\n",
    "        # Store results (including CNN)\n",
    "        results_data[\"delta\"].append(delta)\n",
    "        results_data[\"xgboost_accuracy\"].append(xgboost_accuracy)\n",
    "        results_data[\"random_forest_accuracy\"].append(random_forest_accuracy)\n",
    "        results_data[\"gradient_boosting_accuracy\"].append(gradient_boosting_accuracy)\n",
    "        results_data[\"cnn_accuracy\"].append(cnn_accuracy)  # Add this line\n",
    "        results_data[\"xgboost_time\"].append(xgboost_time)\n",
    "        results_data[\"random_forest_time\"].append(random_forest_time)\n",
    "        results_data[\"gradient_boosting_time\"].append(gradient_boosting_time)\n",
    "        results_data[\"cnn_time\"].append(cnn_time)  # Add this line\n",
    "\n",
    "        print(f\"\\nDelta {delta} Results:\")\n",
    "        print(f\"XGBoost - Accuracy: {xgboost_accuracy:.4f}%, Time: {xgboost_time:.2f}s\")\n",
    "        print(\n",
    "            f\"Random Forest - Accuracy: {random_forest_accuracy:.4f}%, Time: {random_forest_time:.2f}s\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Gradient Boosting - Accuracy: {gradient_boosting_accuracy:.4f}%, Time: {gradient_boosting_time:.2f}s\"\n",
    "        )\n",
    "        print(\n",
    "            f\"CNN - Accuracy: {cnn_accuracy:.4f}%, Time: {cnn_time:.2f}s\"\n",
    "        )  # Add this line\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {pickle_file}. Skipping delta {delta}.\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during delta {delta}: {e}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb3ba07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Final Results DataFrame:\n",
      "============================================================\n",
      "       xgboost_accuracy  random_forest_accuracy  gradient_boosting_accuracy  \\\n",
      "delta                                                                         \n",
      "105           99.947551               81.345688                   95.134744   \n",
      "106           99.940058               77.082345                   92.799520   \n",
      "128           99.955044               80.389121                   95.991408   \n",
      "\n",
      "       cnn_accuracy  xgboost_time  random_forest_time  gradient_boosting_time  \\\n",
      "delta                                                                           \n",
      "105       88.233274      2.453444            2.480797               20.820648   \n",
      "106       89.204210      2.679013            2.107248               18.146357   \n",
      "128       89.613187      5.671495            2.825794               23.377944   \n",
      "\n",
      "         cnn_time  \n",
      "delta              \n",
      "105    294.020774  \n",
      "106    463.251171  \n",
      "128    617.806511  \n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results_data)\n",
    "results_df.set_index(\"delta\", inplace=True)\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(\"Final Results DataFrame:\")\n",
    "print(f\"{'=' * 60}\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b12c8180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results saved as 'model_comparison_results_round_4_with_CNN.pkl' and .csv\n"
     ]
    }
   ],
   "source": [
    "results_df.to_pickle(f\"model_comparison_results_round_{TEST_ROUND}_with_CNN.pkl\")\n",
    "results_df.to_csv(f\"model_comparison_results_round_{TEST_ROUND}_with_CNN.csv\")\n",
    "print(\n",
    "    f\"\\nResults saved as 'model_comparison_results_round_{TEST_ROUND}_with_CNN.pkl' and .csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ae0bf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_pickle(f\"model_comparison_results_round_{TEST_ROUND}_with_CNN.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "160e6c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Training Times:\n",
      "XGBoost: 3.60s\n",
      "Random Forest: 2.47s\n",
      "Gradient Boosting: 20.78s\n",
      "CNN: 458.36s\n"
     ]
    }
   ],
   "source": [
    "avg_times = {\n",
    "    \"XGBoost\": results_df[\"xgboost_time\"].mean(),\n",
    "    \"Random Forest\": results_df[\"random_forest_time\"].mean(),\n",
    "    \"Gradient Boosting\": results_df[\"gradient_boosting_time\"].mean(),\n",
    "    \"CNN\": results_df[\"cnn_time\"].mean(),  # Add this line\n",
    "}\n",
    "\n",
    "print(f\"\\nAverage Training Times:\")\n",
    "for algo, avg_time in avg_times.items():\n",
    "    print(f\"{algo}: {avg_time:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ce77a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"default\")\n",
    "sns.set_palette(\"husl\")\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e77e5f6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create visualizations\n",
    "fig = plt.figure(figsize=(16, 8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35646ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Detailed Comparison Summary\n",
      "================================================================================\n",
      "        Algorithm  Avg Time (s)  Best Accuracy (%)  Worst Accuracy (%)\n",
      "          XGBoost      3.601317          99.955044           99.940058\n",
      "    Random Forest      2.471280          81.345688           77.082345\n",
      "Gradient Boosting     20.781650          95.991408           92.799520\n",
      "              CNN    458.359485          89.613187           88.233274\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"Detailed Comparison Summary\")\n",
    "print(f\"{'=' * 80}\")\n",
    "\n",
    "comparison_summary = pd.DataFrame(\n",
    "    {\n",
    "        \"Algorithm\": [\"XGBoost\", \"Random Forest\", \"Gradient Boosting\", \"CNN\"],\n",
    "        \"Avg Time (s)\": [\n",
    "            avg_times[\"XGBoost\"],\n",
    "            avg_times[\"Random Forest\"],\n",
    "            avg_times[\"Gradient Boosting\"],\n",
    "            avg_times[\"CNN\"],\n",
    "        ],\n",
    "        \"Best Accuracy (%)\": [\n",
    "            max([results_df.loc[d, \"xgboost_accuracy\"] for d in TEST_DELTAS]),\n",
    "            max([results_df.loc[d, \"random_forest_accuracy\"] for d in TEST_DELTAS]),\n",
    "            max([results_df.loc[d, \"gradient_boosting_accuracy\"] for d in TEST_DELTAS]),\n",
    "            max([results_df.loc[d, \"cnn_accuracy\"] for d in TEST_DELTAS]),\n",
    "        ],\n",
    "        \"Worst Accuracy (%)\": [\n",
    "            min([results_df.loc[d, \"xgboost_accuracy\"] for d in TEST_DELTAS]),\n",
    "            min([results_df.loc[d, \"random_forest_accuracy\"] for d in TEST_DELTAS]),\n",
    "            min([results_df.loc[d, \"gradient_boosting_accuracy\"] for d in TEST_DELTAS]),\n",
    "            min([results_df.loc[d, \"cnn_accuracy\"] for d in TEST_DELTAS]),\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(comparison_summary.to_string(index=False))\n",
    "\n",
    "# Update performance rankings to include CNN\n",
    "best_accuracies = [\n",
    "    (\"XGBoost\", max([results_df.loc[d, \"xgboost_accuracy\"] for d in TEST_DELTAS])),\n",
    "    (\n",
    "        \"Random Forest\",\n",
    "        max([results_df.loc[d, \"random_forest_accuracy\"] for d in TEST_DELTAS]),\n",
    "    ),\n",
    "    (\n",
    "        \"Gradient Boosting\",\n",
    "        max([results_df.loc[d, \"gradient_boosting_accuracy\"] for d in TEST_DELTAS]),\n",
    "    ),\n",
    "    (\"CNN\", max([results_df.loc[d, \"cnn_accuracy\"] for d in TEST_DELTAS])),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1519ae61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Performance Rankings\n",
      "============================================================\n",
      "Fastest to Slowest (by average time):\n",
      "1. Random Forest: 2.47s\n",
      "2. XGBoost: 3.60s\n",
      "3. Gradient Boosting: 20.78s\n",
      "4. CNN: 458.36s\n",
      "\n",
      "Best to Worst (by best accuracy achieved):\n",
      "1. XGBoost: 99.9550%\n",
      "2. Gradient Boosting: 95.9914%\n",
      "3. CNN: 89.6132%\n",
      "4. Random Forest: 81.3457%\n"
     ]
    }
   ],
   "source": [
    "# Performance ranking\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(\"Performance Rankings\")\n",
    "print(f\"{'=' * 60}\")\n",
    "\n",
    "print(\"Fastest to Slowest (by average time):\")\n",
    "sorted_by_time = sorted(avg_times.items(), key=lambda x: x[1])\n",
    "for i, (algo, time_val) in enumerate(sorted_by_time, 1):\n",
    "    print(f\"{i}. {algo}: {time_val:.2f}s\")\n",
    "\n",
    "print(\"\\nBest to Worst (by best accuracy achieved):\")\n",
    "sorted_by_accuracy = sorted(best_accuracies, key=lambda x: x[1], reverse=True)\n",
    "for i, (algo, acc) in enumerate(sorted_by_accuracy, 1):\n",
    "    print(f\"{i}. {algo}: {acc:.4f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sm4-xg-boost",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
