{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9045cf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.pkl_preprocessor import PickleBatchLoader\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import pandas as pd\n",
    "\n",
    "# import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from utils.cnn import (\n",
    "    train_cnn_with_batch_loader,\n",
    "    clear_gpu_memory,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee8d5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cnn_incrementally(batch_loader):\n",
    "    \"\"\"\n",
    "    Function to train CNN on batches from a loader.\n",
    "    This integrates CNN training into your existing comparison framework.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting CNN Training ---\")\n",
    "\n",
    "    # Train CNN with the same batch loader\n",
    "    accuracy = train_cnn_with_batch_loader(\n",
    "        batch_loader,\n",
    "        epochs=30,  # Reduced epochs for faster comparison\n",
    "        batch_size=128,  # Smaller batch size for memory efficiency\n",
    "        patience=8,  # Early stopping patience\n",
    "        learning_rate=0.001,\n",
    "    )\n",
    "\n",
    "    # Clear GPU memory after training\n",
    "    clear_gpu_memory()\n",
    "\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4a1cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost_incrementally(batch_loader):\n",
    "    \"\"\"\n",
    "    Function to train XGBoost incrementally on batches from a loader.\n",
    "    This version corrects the incremental training logic.\n",
    "    \"\"\"\n",
    "    # 1. Instantiate the classifier ONCE before the loop.\n",
    "    model = xgb.XGBClassifier(\n",
    "        objective=\"binary:logistic\",\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        n_estimators=100,\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    # This will hold the trained Booster object from the previous iteration.\n",
    "    trained_booster = None\n",
    "\n",
    "    print(\"--- Starting Incremental XGBoost Training ---\")\n",
    "\n",
    "    # 2. Loop through batches\n",
    "    for i, (X_batch, y_batch) in enumerate(batch_loader.batch_generator()):\n",
    "        print(f\"  Training XGBoost on batch {i + 1}/{len(batch_loader)}...\")\n",
    "\n",
    "        # For the first batch, trained_booster is None.\n",
    "        # For subsequent batches, it's the model from the last step.\n",
    "        model.fit(X_batch, y_batch, xgb_model=trained_booster)\n",
    "\n",
    "        # 3. Get the underlying booster to pass to the next iteration\n",
    "        trained_booster = model.get_booster()\n",
    "\n",
    "    # 4. Evaluation on the hold-out test set\n",
    "    print(\"  Evaluating final XGBoost model...\")\n",
    "    X_test, y_test = batch_loader.get_test_set()\n",
    "    predictions = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "    return accuracy * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de6c167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_random_forest_incrementally(batch_loader):\n",
    "    \"\"\"\n",
    "    Function to train Random Forest using warm_start on batches.\n",
    "    \"\"\"\n",
    "    # warm_start=True is key for incremental additions\n",
    "    rf_model = RandomForestClassifier(n_estimators=5, random_state=42, warm_start=True)\n",
    "    print(\"--- Starting Incremental Random Forest Training ---\")\n",
    "\n",
    "    # Training loop\n",
    "    for i, (X_batch, y_batch) in enumerate(batch_loader.batch_generator()):\n",
    "        print(f\"  Training Random Forest on batch {i + 1}/{len(batch_loader)}...\")\n",
    "        rf_model.fit(X_batch, y_batch)\n",
    "        # Increase the number of estimators for the next batch\n",
    "        rf_model.n_estimators += 5\n",
    "\n",
    "    # Evaluation on the hold-out test set\n",
    "    print(\"  Evaluating final Random Forest model...\")\n",
    "    X_test, y_test = batch_loader.get_test_set()\n",
    "    predictions = rf_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    return accuracy * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb809cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gradient_boosting_incrementally(batch_loader):\n",
    "    \"\"\"\n",
    "    Function to train Gradient Boosting using warm_start on batches.\n",
    "    \"\"\"\n",
    "    # warm_start=True is key for incremental additions\n",
    "    gb_model = GradientBoostingClassifier(\n",
    "        n_estimators=5, learning_rate=0.1, max_depth=6, random_state=42, warm_start=True\n",
    "    )\n",
    "    print(\"--- Starting Incremental Gradient Boosting Training ---\")\n",
    "\n",
    "    # Training loop\n",
    "    for i, (X_batch, y_batch) in enumerate(batch_loader.batch_generator()):\n",
    "        print(f\"  Training Gradient Boosting on batch {i + 1}/{len(batch_loader)}...\")\n",
    "        gb_model.fit(X_batch, y_batch)\n",
    "        # Increase the number of estimators for the next batch\n",
    "        gb_model.n_estimators += 5\n",
    "\n",
    "    # Evaluation on the hold-out test set\n",
    "    print(\"  Evaluating final Gradient Boosting model...\")\n",
    "    X_test, y_test = batch_loader.get_test_set()\n",
    "    predictions = gb_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    return accuracy * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695ab453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_algorithm(func, *args, **kwargs):\n",
    "    \"\"\"Utility function to time algorithm execution\"\"\"\n",
    "    start_time = time.time()\n",
    "    result = func(*args, **kwargs)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    return result, execution_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ce1700",
   "metadata": {},
   "source": [
    "# Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee7c6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100096\n",
    "TEST_ROUND = 8\n",
    "TEST_DELTAS = [24, 40, 56]\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"Testing Round {TEST_ROUND} of HIGHT with Deltas: {TEST_DELTAS}\")\n",
    "print(f\"{'=' * 60}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d74b8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_data = {\n",
    "    \"delta\": [],\n",
    "    \"xgboost_accuracy\": [],\n",
    "    \"random_forest_accuracy\": [],\n",
    "    \"gradient_boosting_accuracy\": [],\n",
    "    \"cnn_accuracy\": [],  # Add this line\n",
    "    \"xgboost_time\": [],\n",
    "    \"random_forest_time\": [],\n",
    "    \"gradient_boosting_time\": [],\n",
    "    \"cnn_time\": [],  # Add this line\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c00564",
   "metadata": {},
   "outputs": [],
   "source": [
    "for delta in TEST_DELTAS:\n",
    "    pickle_file = f\"dataset_pkl_round_{TEST_ROUND}/HIGHT_{TEST_ROUND}_round_delta-{delta}_combined.pkl\"\n",
    "\n",
    "    print(f\"\\n{'=' * 50}\")\n",
    "    print(f\"Processing Delta {delta} from file: {pickle_file}\")\n",
    "    print(f\"{'=' * 50}\\n\")\n",
    "\n",
    "    try:\n",
    "        batch_loader = PickleBatchLoader(pickle_file, batch_size=BATCH_SIZE)\n",
    "\n",
    "        # Train and time each algorithm (including CNN)\n",
    "        print(\"Training XGBoost...\")\n",
    "        xgboost_accuracy, xgboost_time = time_algorithm(\n",
    "            train_xgboost_incrementally, batch_loader\n",
    "        )\n",
    "\n",
    "        print(\"Training Random Forest...\")\n",
    "        random_forest_accuracy, random_forest_time = time_algorithm(\n",
    "            train_random_forest_incrementally, batch_loader\n",
    "        )\n",
    "\n",
    "        print(\"Training Gradient Boosting...\")\n",
    "        gradient_boosting_accuracy, gradient_boosting_time = time_algorithm(\n",
    "            train_gradient_boosting_incrementally, batch_loader\n",
    "        )\n",
    "\n",
    "        # Add CNN training\n",
    "        print(\"Training CNN...\")\n",
    "        cnn_accuracy, cnn_time = time_algorithm(train_cnn_incrementally, batch_loader)\n",
    "\n",
    "        # Store results (including CNN)\n",
    "        results_data[\"delta\"].append(delta)\n",
    "        results_data[\"xgboost_accuracy\"].append(xgboost_accuracy)\n",
    "        results_data[\"random_forest_accuracy\"].append(random_forest_accuracy)\n",
    "        results_data[\"gradient_boosting_accuracy\"].append(gradient_boosting_accuracy)\n",
    "        results_data[\"cnn_accuracy\"].append(cnn_accuracy)  # Add this line\n",
    "        results_data[\"xgboost_time\"].append(xgboost_time)\n",
    "        results_data[\"random_forest_time\"].append(random_forest_time)\n",
    "        results_data[\"gradient_boosting_time\"].append(gradient_boosting_time)\n",
    "        results_data[\"cnn_time\"].append(cnn_time)  # Add this line\n",
    "\n",
    "        print(f\"\\nDelta {delta} Results:\")\n",
    "        print(f\"XGBoost - Accuracy: {xgboost_accuracy:.4f}%, Time: {xgboost_time:.2f}s\")\n",
    "        print(\n",
    "            f\"Random Forest - Accuracy: {random_forest_accuracy:.4f}%, Time: {random_forest_time:.2f}s\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Gradient Boosting - Accuracy: {gradient_boosting_accuracy:.4f}%, Time: {gradient_boosting_time:.2f}s\"\n",
    "        )\n",
    "        print(\n",
    "            f\"CNN - Accuracy: {cnn_accuracy:.4f}%, Time: {cnn_time:.2f}s\"\n",
    "        )  # Add this line\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {pickle_file}. Skipping delta {delta}.\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during delta {delta}: {e}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3ba07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results_data)\n",
    "results_df.set_index(\"delta\", inplace=True)\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(\"Final Results DataFrame:\")\n",
    "print(f\"{'=' * 60}\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12c8180",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_pickle(f\"model_comparison_results_round_{TEST_ROUND}_with_CNN.pkl\")\n",
    "results_df.to_csv(f\"model_comparison_results_round_{TEST_ROUND}_with_CNN.csv\")\n",
    "print(\n",
    "    f\"\\nResults saved as 'model_comparison_results_round_{TEST_ROUND}_with_CNN.pkl' and .csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae0bf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_pickle(f\"model_comparison_results_round_{TEST_ROUND}_with_CNN.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160e6c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_times = {\n",
    "    \"XGBoost\": results_df[\"xgboost_time\"].mean(),\n",
    "    \"Random Forest\": results_df[\"random_forest_time\"].mean(),\n",
    "    \"Gradient Boosting\": results_df[\"gradient_boosting_time\"].mean(),\n",
    "    \"CNN\": results_df[\"cnn_time\"].mean(),  # Add this line\n",
    "}\n",
    "\n",
    "print(f\"\\nAverage Training Times:\")\n",
    "for algo, avg_time in avg_times.items():\n",
    "    print(f\"{algo}: {avg_time:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce77a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"default\")\n",
    "sns.set_palette(\"husl\")\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77e5f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "fig = plt.figure(figsize=(16, 8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35646ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"Detailed Comparison Summary\")\n",
    "print(f\"{'=' * 80}\")\n",
    "\n",
    "comparison_summary = pd.DataFrame(\n",
    "    {\n",
    "        \"Algorithm\": [\"XGBoost\", \"Random Forest\", \"Gradient Boosting\", \"CNN\"],\n",
    "        \"Avg Time (s)\": [\n",
    "            avg_times[\"XGBoost\"],\n",
    "            avg_times[\"Random Forest\"],\n",
    "            avg_times[\"Gradient Boosting\"],\n",
    "            avg_times[\"CNN\"],\n",
    "        ],\n",
    "        \"Best Accuracy (%)\": [\n",
    "            max([results_df.loc[d, \"xgboost_accuracy\"] for d in TEST_DELTAS]),\n",
    "            max([results_df.loc[d, \"random_forest_accuracy\"] for d in TEST_DELTAS]),\n",
    "            max([results_df.loc[d, \"gradient_boosting_accuracy\"] for d in TEST_DELTAS]),\n",
    "            max([results_df.loc[d, \"cnn_accuracy\"] for d in TEST_DELTAS]),\n",
    "        ],\n",
    "        \"Worst Accuracy (%)\": [\n",
    "            min([results_df.loc[d, \"xgboost_accuracy\"] for d in TEST_DELTAS]),\n",
    "            min([results_df.loc[d, \"random_forest_accuracy\"] for d in TEST_DELTAS]),\n",
    "            min([results_df.loc[d, \"gradient_boosting_accuracy\"] for d in TEST_DELTAS]),\n",
    "            min([results_df.loc[d, \"cnn_accuracy\"] for d in TEST_DELTAS]),\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(comparison_summary.to_string(index=False))\n",
    "\n",
    "# Update performance rankings to include CNN\n",
    "best_accuracies = [\n",
    "    (\"XGBoost\", max([results_df.loc[d, \"xgboost_accuracy\"] for d in TEST_DELTAS])),\n",
    "    (\n",
    "        \"Random Forest\",\n",
    "        max([results_df.loc[d, \"random_forest_accuracy\"] for d in TEST_DELTAS]),\n",
    "    ),\n",
    "    (\n",
    "        \"Gradient Boosting\",\n",
    "        max([results_df.loc[d, \"gradient_boosting_accuracy\"] for d in TEST_DELTAS]),\n",
    "    ),\n",
    "    (\"CNN\", max([results_df.loc[d, \"cnn_accuracy\"] for d in TEST_DELTAS])),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1519ae61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance ranking\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(\"Performance Rankings\")\n",
    "print(f\"{'=' * 60}\")\n",
    "\n",
    "print(\"Fastest to Slowest (by average time):\")\n",
    "sorted_by_time = sorted(avg_times.items(), key=lambda x: x[1])\n",
    "for i, (algo, time_val) in enumerate(sorted_by_time, 1):\n",
    "    print(f\"{i}. {algo}: {time_val:.2f}s\")\n",
    "\n",
    "print(\"\\nBest to Worst (by best accuracy achieved):\")\n",
    "sorted_by_accuracy = sorted(best_accuracies, key=lambda x: x[1], reverse=True)\n",
    "for i, (algo, acc) in enumerate(sorted_by_accuracy, 1):\n",
    "    print(f\"{i}. {algo}: {acc:.4f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hight-xg-boost",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
