{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6276bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.pkl_preprocessor import PickleBatchLoader\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a6ab17",
   "metadata": {},
   "source": [
    "# Using XG-Boost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad804c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost_incrementally(batch_loader):\n",
    "    \"\"\"\n",
    "    Function to train XGBoost incrementally on batches from a loader.\n",
    "    This version corrects the incremental training logic.\n",
    "    \"\"\"\n",
    "    # 1. Instantiate the classifier ONCE before the loop.\n",
    "    model = xgb.XGBClassifier(\n",
    "        objective=\"binary:logistic\",\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        n_estimators=100,\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    # This will hold the trained Booster object from the previous iteration.\n",
    "    trained_booster = None\n",
    "\n",
    "    print(\"--- Starting Incremental XGBoost Training ---\")\n",
    "\n",
    "    # 2. Loop through batches\n",
    "    for i, (X_batch, y_batch) in enumerate(batch_loader.batch_generator()):\n",
    "        print(f\"  Training XGBoost on batch {i + 1}/{len(batch_loader)}...\")\n",
    "\n",
    "        # For the first batch, trained_booster is None.\n",
    "        # For subsequent batches, it's the model from the last step.\n",
    "        model.fit(X_batch, y_batch, xgb_model=trained_booster)\n",
    "\n",
    "        # 3. Get the underlying booster to pass to the next iteration\n",
    "        trained_booster = model.get_booster()\n",
    "\n",
    "    # 4. Evaluation on the hold-out test set\n",
    "    print(\"  Evaluating final XGBoost model...\")\n",
    "    X_test, y_test = batch_loader.get_test_set()\n",
    "    predictions = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "    return accuracy * 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4510ed5",
   "metadata": {},
   "source": [
    "# Using Random Forset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc337d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_random_forest_incrementally(batch_loader):\n",
    "    \"\"\"\n",
    "    Function to train Random Forest using warm_start on batches.\n",
    "    \"\"\"\n",
    "    # warm_start=True is key for incremental additions\n",
    "    rf_model = RandomForestClassifier(n_estimators=5, random_state=42, warm_start=True)\n",
    "    print(\"--- Starting Incremental Random Forest Training ---\")\n",
    "\n",
    "    # Training loop\n",
    "    for i, (X_batch, y_batch) in enumerate(batch_loader.batch_generator()):\n",
    "        print(f\"  Training Random Forest on batch {i + 1}/{len(batch_loader)}...\")\n",
    "        rf_model.fit(X_batch, y_batch)\n",
    "        # Increase the number of estimators for the next batch\n",
    "        rf_model.n_estimators += 5\n",
    "\n",
    "    # Evaluation on the hold-out test set\n",
    "    print(\"  Evaluating final Random Forest model...\")\n",
    "    X_test, y_test = batch_loader.get_test_set()\n",
    "    predictions = rf_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    return accuracy * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f79a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def use_gradient_boosting(X_train, y_train_encoded, X_test, y_test_encoded):\n",
    "#     \"\"\"\n",
    "#     Function to use Gradient Boosting for classification.\n",
    "#     \"\"\"\n",
    "#     model = GradientBoostingClassifier(\n",
    "#         n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42\n",
    "#     )\n",
    "\n",
    "#     model.fit(X_train, y_train_encoded)\n",
    "\n",
    "#     predictions = model.predict(X_test)\n",
    "\n",
    "#     accuracy = accuracy_score(y_test_encoded, predictions)\n",
    "\n",
    "#     return accuracy * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a9c9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size\n",
    "BATCH_SIZE = 100096\n",
    "\n",
    "\n",
    "for rounds in range(10, 11, 1):\n",
    "    print(\"\\n========================================================\")\n",
    "    print(f\"Processing Round {rounds} from file\")\n",
    "    print(\"========================================================\\n\")\n",
    "    rounds_list = []\n",
    "    xgboost_accuracies = []\n",
    "    random_forest_accuracies = []\n",
    "    delta_list = []\n",
    "    for delta in range(1, 65, 1):\n",
    "        pickle_file = f\"dataset_pkl_round_{rounds}/HIGHT_{rounds}_round_delta-{delta}_combined.pkl\"\n",
    "        print(\"\\n========================================================\")\n",
    "        print(f\"Processing Delta {delta}/64 from file: {pickle_file}\")\n",
    "        print(\"========================================================\\n\")\n",
    "\n",
    "        try:\n",
    "            batch_loader = PickleBatchLoader(pickle_file, batch_size=BATCH_SIZE)\n",
    "\n",
    "            xgboost_accuracy = train_xgboost_incrementally(batch_loader)\n",
    "            # random_forest_accuracy = train_random_forest_incrementally(batch_loader)\n",
    "\n",
    "            delta_list.append(delta)\n",
    "            rounds_list.append(rounds)\n",
    "            xgboost_accuracies.append(xgboost_accuracy)\n",
    "            # random_forest_accuracies.append(random_forest_accuracy)\n",
    "\n",
    "            print(\n",
    "                f\"\\nRound {rounds} Completed | \"\n",
    "                f\"Final XGBoost Accuracy: {xgboost_accuracy:.4f}% | \"\n",
    "                # f\"Final Random Forest Accuracy: {random_forest_accuracy:.4f}%\\n\"\n",
    "            )\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {pickle_file}. Skipping round {rounds}.\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during round {rounds}: {e}\")\n",
    "            continue\n",
    "\n",
    "    results_df = pd.DataFrame(\n",
    "        {\n",
    "            \"Deltas\": delta_list,\n",
    "            \"XGBoost_Accuracy\": xgboost_accuracies,\n",
    "            # \"RandomForest_Accuracy\": random_forest_accuracies,\n",
    "        }\n",
    "    )\n",
    "    results_df.set_index(\"Deltas\", inplace=True)\n",
    "\n",
    "    print(\"\\nFinal Results DataFrame:\")\n",
    "    print(results_df)\n",
    "    results_df.to_pickle(f\"model_accuracy_results_incremental_round_{rounds}.pkl\")\n",
    "    results_df.to_csv(f\"model_accuracy_results_incremental_round_{rounds}.csv\")\n",
    "    print(\n",
    "        f\"\\nDataFrame saved as 'model_accuracy_results_incremental_round_{rounds}.pkl' and .csv\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5251004a",
   "metadata": {},
   "source": [
    "# Code to Plot Graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5d266c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf875c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "round = 9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079a39ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_pickle(f\"model_accuracy_results_incremental_round_{round}.pkl\")\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dd9538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style for clean, aesthetic look\n",
    "plt.style.use(\"default\")  # Reset to default style\n",
    "sns.set_palette(\"husl\")  # Use a more aesthetic color palette\n",
    "\n",
    "plt.figure(figsize=(20, 8), facecolor=\"white\")\n",
    "plot_df = results_df.reset_index()\n",
    "\n",
    "# Create the plot with enhanced aesthetics\n",
    "ax = plt.gca()\n",
    "ax.set_facecolor(\"white\")  # Ensure plot background is white\n",
    "\n",
    "# Plot XGBoost with enhanced styling\n",
    "sns.lineplot(\n",
    "    data=plot_df,\n",
    "    x=\"Deltas\",\n",
    "    y=\"XGBoost_Accuracy\",\n",
    "    marker=\"o\",\n",
    "    label=\"XGBoost\",\n",
    "    linewidth=2.5,\n",
    "    markersize=6,\n",
    "    color=\"#2E86AB\",  # Professional blue color\n",
    ")\n",
    "\n",
    "# Uncomment these for other models with aesthetic colors\n",
    "# sns.lineplot(\n",
    "#     data=plot_df,\n",
    "#     x=\"Deltas\",\n",
    "#     y=\"RandomForest_Accuracy\",\n",
    "#     marker=\"s\",\n",
    "#     label=\"Random Forest\",\n",
    "#     linewidth=2.5,\n",
    "#     markersize=6,\n",
    "#     color='#A23B72'  # Professional magenta\n",
    "# )\n",
    "# sns.lineplot(\n",
    "#     data=plot_df,\n",
    "#     x=\"Deltas\",\n",
    "#     y=\"GradientBoosting_Accuracy\",\n",
    "#     marker=\"^\",\n",
    "#     label=\"Gradient Boosting\",\n",
    "#     linewidth=2.5,\n",
    "#     markersize=6,\n",
    "#     color='#F18F01'  # Professional orange\n",
    "# )\n",
    "\n",
    "# Enhanced title and labels\n",
    "plt.title(\n",
    "    \"Model Accuracy Comparison (Incremental Training)\",\n",
    "    fontsize=18,\n",
    "    fontweight=\"bold\",\n",
    "    color=\"#2c3e50\",  # Dark blue-gray color\n",
    "    pad=20,\n",
    ")\n",
    "plt.xlabel(\"Deltas\", fontsize=14, fontweight=\"medium\", color=\"#34495e\")\n",
    "plt.ylabel(\"Accuracy\", fontsize=14, fontweight=\"medium\", color=\"#34495e\")\n",
    "\n",
    "# Enhanced legend\n",
    "plt.legend(\n",
    "    fontsize=12,\n",
    "    frameon=True,\n",
    "    fancybox=True,\n",
    "    shadow=True,\n",
    "    facecolor=\"white\",\n",
    "    edgecolor=\"lightgray\",\n",
    "    framealpha=0.9,\n",
    ")\n",
    "\n",
    "# Improved grid\n",
    "plt.grid(True, alpha=0.3, linestyle=\"-\", linewidth=0.5, color=\"gray\")\n",
    "\n",
    "# Enhanced tick styling\n",
    "plt.xticks(range(1, 65, 1), fontsize=10, color=\"#2c3e50\")\n",
    "plt.yticks(fontsize=10, color=\"#2c3e50\")\n",
    "\n",
    "# Remove top and right spines for cleaner look\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.spines[\"left\"].set_color(\"#bdc3c7\")\n",
    "ax.spines[\"bottom\"].set_color(\"#bdc3c7\")\n",
    "\n",
    "# Set white background for the entire figure\n",
    "plt.gcf().patch.set_facecolor(\"white\")\n",
    "\n",
    "# Reduce whitespace around the plot\n",
    "plt.margins(x=0.02, y=0.02)\n",
    "plt.tight_layout(pad=2.0)\n",
    "\n",
    "# Save with white background\n",
    "plt.savefig(\n",
    "    f\"model_accuracy_comparison_incremental_round_{round}.png\",\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    "    facecolor=\"white\",  # Ensure saved image has white background\n",
    "    edgecolor=\"none\",\n",
    ")\n",
    "plt.show()\n",
    "print(\n",
    "    f\"Enhanced plot saved as 'model_accuracy_comparison_incremental_round_{round}.png'\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9134da62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming your DataFrame is called results_df\n",
    "# and it already has columns: XGBoost_Accuracy, RandomForest_Accuracy\n",
    "\n",
    "# Create a new column for the mean of the two\n",
    "results_df[\"Mean_Accuracy\"] = results_df[\n",
    "    [\"XGBoost_Accuracy\", \"RandomForest_Accuracy\"]\n",
    "].mean(axis=1)\n",
    "\n",
    "# Sort by XGBoost_Accuracy, then RandomForest_Accuracy, then Mean_Accuracy\n",
    "sorted_df = results_df.sort_values(\n",
    "    by=[\"XGBoost_Accuracy\", \"RandomForest_Accuracy\", \"Mean_Accuracy\"],\n",
    "    ascending=[False, False, False],\n",
    ")\n",
    "sorted_df.head()  # show top rows\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hight-xg-boost",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
