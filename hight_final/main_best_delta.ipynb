{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6276bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.pkl_preprocessor import PickleBatchLoader\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a6ab17",
   "metadata": {},
   "source": [
    "# Using XG-Boost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad804c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost_incrementally(batch_loader):\n",
    "    \"\"\"\n",
    "    Function to train XGBoost incrementally on batches from a loader.\n",
    "    This version corrects the incremental training logic.\n",
    "    \"\"\"\n",
    "    # 1. Instantiate the classifier ONCE before the loop.\n",
    "    model = xgb.XGBClassifier(\n",
    "        objective=\"binary:logistic\",\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        n_estimators=100,\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    # This will hold the trained Booster object from the previous iteration.\n",
    "    trained_booster = None\n",
    "\n",
    "    print(\"--- Starting Incremental XGBoost Training ---\")\n",
    "\n",
    "    # 2. Loop through batches\n",
    "    for i, (X_batch, y_batch) in enumerate(batch_loader.batch_generator()):\n",
    "        print(f\"  Training XGBoost on batch {i + 1}/{len(batch_loader)}...\")\n",
    "\n",
    "        # For the first batch, trained_booster is None.\n",
    "        # For subsequent batches, it's the model from the last step.\n",
    "        model.fit(X_batch, y_batch, xgb_model=trained_booster)\n",
    "\n",
    "        # 3. Get the underlying booster to pass to the next iteration\n",
    "        trained_booster = model.get_booster()\n",
    "\n",
    "    # 4. Evaluation on the hold-out test set\n",
    "    print(\"  Evaluating final XGBoost model...\")\n",
    "    X_test, y_test = batch_loader.get_test_set()\n",
    "    predictions = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "    return accuracy * 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4510ed5",
   "metadata": {},
   "source": [
    "# Using Random Forset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc337d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_random_forest_incrementally(batch_loader):\n",
    "    \"\"\"\n",
    "    Function to train Random Forest using warm_start on batches.\n",
    "    \"\"\"\n",
    "    # warm_start=True is key for incremental additions\n",
    "    rf_model = RandomForestClassifier(n_estimators=5, random_state=42, warm_start=True)\n",
    "    print(\"--- Starting Incremental Random Forest Training ---\")\n",
    "\n",
    "    # Training loop\n",
    "    for i, (X_batch, y_batch) in enumerate(batch_loader.batch_generator()):\n",
    "        print(f\"  Training Random Forest on batch {i + 1}/{len(batch_loader)}...\")\n",
    "        rf_model.fit(X_batch, y_batch)\n",
    "        # Increase the number of estimators for the next batch\n",
    "        rf_model.n_estimators += 5\n",
    "\n",
    "    # Evaluation on the hold-out test set\n",
    "    print(\"  Evaluating final Random Forest model...\")\n",
    "    X_test, y_test = batch_loader.get_test_set()\n",
    "    predictions = rf_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    return accuracy * 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613ecc73",
   "metadata": {},
   "source": [
    "# Using Gradient Boosting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f79a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gradient_boosting_incrementally(batch_loader):\n",
    "    \"\"\"\n",
    "    Function to train Gradient Boosting using warm_start on batches.\n",
    "    \"\"\"\n",
    "    # warm_start=True is key for incremental additions\n",
    "    gb_model = GradientBoostingClassifier(\n",
    "        n_estimators=5, learning_rate=0.1, max_depth=6, random_state=42, warm_start=True\n",
    "    )\n",
    "    print(\"--- Starting Incremental Gradient Boosting Training ---\")\n",
    "\n",
    "    # Training loop\n",
    "    for i, (X_batch, y_batch) in enumerate(batch_loader.batch_generator()):\n",
    "        print(f\"  Training Gradient Boosting on batch {i + 1}/{len(batch_loader)}...\")\n",
    "        gb_model.fit(X_batch, y_batch)\n",
    "        # Increase the number of estimators for the next batch\n",
    "        gb_model.n_estimators += 5\n",
    "\n",
    "    # Evaluation on the hold-out test set\n",
    "    print(\"  Evaluating final Gradient Boosting model...\")\n",
    "    X_test, y_test = batch_loader.get_test_set()\n",
    "    predictions = gb_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    return accuracy * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8afef6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d58b290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.hight import hight_encryption_binary\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "mk = random.getrandbits(128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc61099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for rounds in range(1, 33, 1):\n",
    "#     PICKLE_DIR = Path(f\"HIGHT_Output_Flipped_Bits_round_{rounds}\")\n",
    "#     PICKLE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "#     print(\"=========================================================\")\n",
    "#     print(f\"Processing round {rounds}...\")\n",
    "#     print(\"=========================================================\")\n",
    "\n",
    "#     all_rows_data = []\n",
    "#     df = pd.read_pickle(f\"flipped_bits_raw/bit_flipped_delta-{delta}.pkl\")\n",
    "#     P1 = df[\"P1\"].values\n",
    "#     P2 = df[\"P2\"].values\n",
    "#     for i in range(100096):\n",
    "#         plain_text_1 = P1[i]\n",
    "#         plain_text_2 = P2[i]\n",
    "#         C1 = hight_encryption_binary(plain_text_1, mk, rounds)\n",
    "#         C2 = hight_encryption_binary(plain_text_2, mk, rounds)\n",
    "#         C = result = np.concatenate([C1, C2])\n",
    "#         all_rows_data.append({\"Encrypted_data\": C, \"Class\": \"flipped_bits\"})\n",
    "#     df = pd.DataFrame(all_rows_data)\n",
    "#     df.to_pickle(\n",
    "#         f\"HIGHT_Output_Flipped_Bits_round_{rounds}/HIGHT_{rounds}_round_delta-{delta}.pkl\"\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ff2386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for rounds in range(1, 33, 1):\n",
    "#     DIR = Path(f\"dataset_pkl_round_{rounds}\")\n",
    "#     DIR.mkdir(exist_ok=True)\n",
    "#     df_hight_flipped = pd.read_pickle(\n",
    "#         f\"HIGHT_Output_Flipped_Bits_round_{rounds}/HIGHT_{rounds}_round_delta-{delta}.pkl\"\n",
    "#     )\n",
    "#     df_hight_diff = pd.read_pickle(f\"HIGHT_Output_Diff_Bits/HIGHT_{rounds}_round.pkl\")\n",
    "\n",
    "#     df_combined = pd.concat([df_hight_flipped, df_hight_diff], ignore_index=True)\n",
    "#     df_combined.to_pickle(\n",
    "#         f\"dataset_pkl_round_{rounds}/HIGHT_{rounds}_round_delta-{delta}_combined.pkl\"\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a9c9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100096\n",
    "rounds_list = []\n",
    "xgboost_accuracies = []\n",
    "random_forest_accuracies = []\n",
    "gradient_boosting_accuracies = []\n",
    "for rounds in range(1, 33, 1):\n",
    "    print(\"\\n========================================================\")\n",
    "    print(f\"Processing Round {rounds} from file\")\n",
    "    print(\"========================================================\\n\")\n",
    "    pickle_file = (\n",
    "        f\"dataset_pkl_round_{rounds}/HIGHT_{rounds}_round_delta-{delta}_combined.pkl\"\n",
    "    )\n",
    "    try:\n",
    "        batch_loader = PickleBatchLoader(pickle_file, batch_size=BATCH_SIZE)\n",
    "        xgboost_accuracy = train_xgboost_incrementally(batch_loader)\n",
    "        random_forest_accuracy = train_random_forest_incrementally(batch_loader)\n",
    "        gradient_boosting_accuracy = train_gradient_boosting_incrementally(batch_loader)\n",
    "\n",
    "        rounds_list.append(rounds)\n",
    "        xgboost_accuracies.append(xgboost_accuracy)\n",
    "        random_forest_accuracies.append(random_forest_accuracy)\n",
    "        gradient_boosting_accuracies.append(gradient_boosting_accuracy)\n",
    "        print(\n",
    "            f\"\\nRound {rounds} Completed | \"\n",
    "            f\"Final XGBoost Accuracy: {xgboost_accuracy:.4f}%\\n\"\n",
    "            f\"Final Random Forest Accuracy: {random_forest_accuracy:.4f}%\\n\"\n",
    "            f\"Final Gradient Boosting Accuracy: {gradient_boosting_accuracy:.4f}%\\n\"\n",
    "        )\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {pickle_file}. Skipping round {rounds}.\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during round {rounds}: {e}\")\n",
    "        continue\n",
    "\n",
    "results_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Rounds\": rounds_list,\n",
    "        \"XGBoost_Accuracy\": xgboost_accuracies,\n",
    "        \"RandomForest_Accuracy\": random_forest_accuracies,\n",
    "        \"GradientBoosting_Accuracy\": gradient_boosting_accuracies,\n",
    "    }\n",
    ")\n",
    "results_df.set_index(\"Rounds\", inplace=True)\n",
    "print(\"\\nFinal Results DataFrame:\")\n",
    "print(results_df)\n",
    "results_df.to_pickle(f\"model_accuracy_results_incremental_delta_{delta}.pkl\")\n",
    "results_df.to_csv(f\"model_accuracy_results_incremental_delta_{delta}.csv\")\n",
    "print(\n",
    "    f\"\\nDataFrame saved as 'model_accuracy_results_incremental_delta_{delta}.pkl' and .csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5251004a",
   "metadata": {},
   "source": [
    "# Code to Plot Graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6759ddb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb285dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_pickle(f\"model_accuracy_results_incremental_delta_{delta}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dd9538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style for clean, aesthetic look\n",
    "plt.style.use(\"default\")  # Reset to default style\n",
    "sns.set_palette(\"husl\")  # Use a more aesthetic color palette\n",
    "\n",
    "plt.figure(figsize=(11, 10), facecolor=\"white\")\n",
    "plot_df = results_df.reset_index()\n",
    "\n",
    "# Add this line after creating plot_df\n",
    "plot_df_filtered = plot_df[plot_df[\"Rounds\"] <= 11]\n",
    "\n",
    "# Create the plot with enhanced aesthetics\n",
    "ax = plt.gca()\n",
    "ax.set_facecolor(\"white\")  # Ensure plot background is white\n",
    "\n",
    "# Plot XGBoost with enhanced styling\n",
    "sns.lineplot(\n",
    "    data=plot_df_filtered,\n",
    "    x=\"Rounds\",\n",
    "    y=\"XGBoost_Accuracy\",\n",
    "    marker=\"o\",\n",
    "    label=\"XGBoost\",\n",
    "    linewidth=2.5,\n",
    "    markersize=6,\n",
    "    color=\"#2E86AB\",  # Professional blue color\n",
    ")\n",
    "\n",
    "sns.lineplot(\n",
    "    data=plot_df_filtered,\n",
    "    x=\"Rounds\",\n",
    "    y=\"RandomForest_Accuracy\",\n",
    "    marker=\"s\",\n",
    "    label=\"Random Forest\",\n",
    "    linewidth=2.5,\n",
    "    markersize=6,\n",
    "    color=\"#A23B72\",  # Professional magenta\n",
    ")\n",
    "sns.lineplot(\n",
    "    data=plot_df_filtered,\n",
    "    x=\"Rounds\",\n",
    "    y=\"GradientBoosting_Accuracy\",\n",
    "    marker=\"^\",\n",
    "    label=\"Gradient Boosting\",\n",
    "    linewidth=2.5,\n",
    "    markersize=6,\n",
    "    color=\"#F18F01\",  # Professional orange\n",
    ")\n",
    "\n",
    "# Enhanced title and labels\n",
    "plt.title(\n",
    "    \"Model Accuracy Comparison (Incremental Training)\",\n",
    "    fontsize=18,\n",
    "    fontweight=\"bold\",\n",
    "    color=\"#2c3e50\",  # Dark blue-gray color\n",
    "    pad=20,\n",
    ")\n",
    "plt.xlabel(\"Rounds\", fontsize=14, fontweight=\"medium\", color=\"#34495e\")\n",
    "plt.ylabel(\"Accuracy\", fontsize=14, fontweight=\"medium\", color=\"#34495e\")\n",
    "\n",
    "# Enhanced legend\n",
    "plt.legend(\n",
    "    fontsize=12,\n",
    "    frameon=True,\n",
    "    fancybox=True,\n",
    "    shadow=True,\n",
    "    facecolor=\"white\",\n",
    "    edgecolor=\"lightgray\",\n",
    "    framealpha=0.9,\n",
    ")\n",
    "\n",
    "# Improved grid\n",
    "plt.grid(True, alpha=0.3, linestyle=\"-\", linewidth=0.5, color=\"gray\")\n",
    "\n",
    "# Enhanced tick styling\n",
    "plt.xticks(range(1, 12, 1), fontsize=10, color=\"#2c3e50\")\n",
    "plt.yticks(fontsize=10, color=\"#2c3e50\")\n",
    "\n",
    "# Remove top and right spines for cleaner look\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.spines[\"left\"].set_color(\"#bdc3c7\")\n",
    "ax.spines[\"bottom\"].set_color(\"#bdc3c7\")\n",
    "\n",
    "# Set white background for the entire figure\n",
    "plt.gcf().patch.set_facecolor(\"white\")\n",
    "\n",
    "# Reduce whitespace around the plot\n",
    "plt.margins(x=0.02, y=0.02)\n",
    "plt.tight_layout(pad=2.0)\n",
    "\n",
    "# Save with white background\n",
    "plt.savefig(\n",
    "    f\"model_accuracy_comparison_incremental_delta-{delta}.png\",\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    "    facecolor=\"white\",  # Ensure saved image has white background\n",
    "    edgecolor=\"none\",\n",
    ")\n",
    "plt.show()\n",
    "print(\n",
    "    f\"Enhanced plot saved as 'model_accuracy_comparison_incremental_delta-{delta}.png'\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hight-xg-boost",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
